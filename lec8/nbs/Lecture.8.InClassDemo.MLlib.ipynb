{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[3.0,1.0,38.0,9.0...|  1.0|       1.0|\n",
      "|[3.0,2.0,38.0,10....|  0.0|       0.0|\n",
      "|[3.0,3.0,38.0,15....|  0.0|       0.0|\n",
      "|[5.0,2.0,38.0,12....|  0.0|       0.0|\n",
      "|[5.0,4.0,38.0,13....|  0.0|       0.0|\n",
      "|[6.0,1.0,38.0,13....|  1.0|       1.0|\n",
      "|[7.0,4.0,37.0,12....|  0.0|       0.0|\n",
      "|[9.0,2.0,35.0,14....|  0.0|       0.0|\n",
      "|[9.0,3.0,36.0,12....|  0.0|       0.0|\n",
      "|[11.0,2.0,35.0,10...|  0.0|       0.0|\n",
      "|[12.0,3.0,34.0,12...|  0.0|       0.0|\n",
      "|[14.0,1.0,33.0,18...|  1.0|       1.0|\n",
      "|[14.0,2.0,33.0,12...|  0.0|       0.0|\n",
      "|[15.0,2.0,32.0,12...|  0.0|       0.0|\n",
      "|[17.0,2.0,68.0,8....|  0.0|       0.0|\n",
      "|[20.0,2.0,62.0,12...|  0.0|       0.0|\n",
      "|[21.0,1.0,61.0,8....|  1.0|       1.0|\n",
      "|[21.0,4.0,61.0,16...|  0.0|       0.0|\n",
      "|[22.0,1.0,61.0,11...|  1.0|       1.0|\n",
      "|[22.0,2.0,62.0,9....|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "dot: (s: String)Double\n",
       "readingsRDD: org.apache.spark.rdd.RDD[String] = breastcancer.txt MapPartitionsRDD[12] at textFile at <console>:50\n",
       "RDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[13] at map at <console>:51\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[14] at map at <console>:52\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(S...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 1: logistic regression\n",
    "\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "/**\n",
    "*  Missing values show up as a dot.  The dot function below\n",
    "*  returns -1 if there is a dot or blank value. And it converts strings to double.\n",
    "* Then later we will\n",
    "*  delete all rows that have any -1 values.\n",
    "*/\n",
    "def dot (s: String) : Double = {\n",
    "if (s.contains(\".\") || s.length == 0) {\n",
    "return -1\n",
    "} else {\n",
    "return s.toDouble\n",
    "}\n",
    "}\n",
    "// To create DataFrame\n",
    "// Step 1. create an RDD of Rows\n",
    "val readingsRDD = sc.textFile(\"breastcancer.txt\")\n",
    "val RDD = readingsRDD.map(_.split(\",\"))\n",
    "val rowRDD = RDD.map(s => Row(dot(s(0)),dot(s(1)),dot(s(2)),dot(s(3)),dot(s(4)),dot(s(5)),dot(s(6)),\n",
    "dot(s(7)),dot(s(8)),dot(s(9)),dot(s(10)),dot(s(11)),dot(s(12)),\n",
    "dot(s(13))))\n",
    "\n",
    "// Step 2. create a schema\n",
    "val schema = StructType (List(\n",
    "StructField(\"STR\", DoubleType, true),\n",
    "StructField(\"OBS\", DoubleType, true),\n",
    "StructField(\"AGMT\", DoubleType, true),\n",
    "StructField(\"FNDX\", DoubleType, true),\n",
    "StructField(\"HIGD\", DoubleType, true),\n",
    "StructField(\"DEG\",DoubleType, true),\n",
    "StructField(\"CHK\", DoubleType, true),\n",
    "StructField(\"AGP1\", DoubleType, true),\n",
    "StructField(\"AGMN\", DoubleType, true),\n",
    "StructField(\"NLV\", DoubleType, true),\n",
    "StructField(\"LIV\", DoubleType, true),\n",
    "StructField(\"WT\", DoubleType, true),\n",
    "StructField(\"AGLP\", DoubleType, true),\n",
    "StructField(\"MST\", DoubleType, true)))\n",
    "\n",
    "// Step 3. Apply schema to RDD of Rows\n",
    "val readingsDF = spark.createDataFrame(rowRDD, schema)\n",
    "// readingsDF.show(10)\n",
    "/**\n",
    "*  Create a new dataframe dropping all of those with missing values.\n",
    "*/\n",
    "\n",
    "//var cleanDF = readingsDF.filter(readingsDF(\"NLV\") > -1)\n",
    "var cleanDF = readingsDF.filter($\"STR\" > -1 && $\"OBS\" > -1 && $\"AGMT\" > -1 && $\"FNDX\" > -1 \n",
    "                                && $\"HIGD\" > -1 && $\"DEG\" > -1 && $\"CHK\" > -1 && $\"AGP1\" > -1\n",
    "                                && $\"AGMN\" > -1 && $\"NLV\" > -1 && $\"LIV\" > -1 && $\"WT\" > -1\n",
    "                                && $\"AGLP\" > -1 && $\"MST\" > -1)\n",
    "\n",
    "//cleanDF.show(10)\n",
    "\n",
    "/**\n",
    "*  Now comes something more complicated.  Our dataframe has the column headings\n",
    "*  we created with the schema.  But we need a column called “label” and one called\n",
    "* “features” to plug into the LR algorithm.  So we use the VectorAssembler() to do that.\n",
    "* Features is a Vector of doubles.  These are all the values like patient age, etc. that\n",
    "* we extracted above.  The label indicated whether the patient has cancer.\n",
    "*/\n",
    "val featureCols = Array(\"STR\" , \"OBS\" , \"AGMT\" , \"HIGD\" , \"DEG\" , \"CHK\" , \"AGP1\" , \"AGMN\" , \"NLV\" , \"LIV\" , \"WT\" , \"AGLP\",  \"MST\" )\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "val df2 = assembler.transform(cleanDF)\n",
    "\n",
    "\n",
    "// df2.show(10)\n",
    "\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"FNDX\").setOutputCol(\"label\")\n",
    "val df3 = labelIndexer.fit(df2).transform(df2)\n",
    "\n",
    "val Array(dfTrain,dfTest) = df3.randomSplit(Array(0.7,0.3))\n",
    "/**\n",
    "*   Now we declare the LR model and run fit and transform to make predictions.\n",
    "*/\n",
    "val lrModel = new LogisticRegression().fit(dfTrain)\n",
    "val predictions = lrModel.transform(dfTest)\n",
    "\n",
    "predictions.select (\"features\", \"label\", \"prediction\").show()\n",
    "val lrEvaluator = new BinaryClassificationEvaluator()                    \n",
    "                    .setRawPredictionCol(\"rawPrediction\")\n",
    "val p = lrEvaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC: $p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strIdx_5ae239708d0f\n",
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[98,99,100,1...|\n",
      "|           0.0|  0.0|(692,[121,122,123...|\n",
      "|           0.0|  0.0|(692,[122,123,148...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           1.0|  0.0|(692,[125,126,127...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.08333333333333337\n",
      "Learned classification tree model:\n",
      " DecisionTreeClassificationModel (uid=dtc_56e670285563) of depth 2 with 5 nodes\n",
      "  If (feature 405 <= 21.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 405 > 21.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = [label: double, features: vector]\n",
       "labelIndexer = strIdx_5ae239708d0f\n",
       "featureIndexer = vecIdx_db66e2ab4645\n",
       "trainingData = [label: double, features: vector]\n",
       "testData = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.m...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 2: Decision Tree\n",
    "// with pipeline\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "// Load the data stored in LIBSVM format as a DataFrame.\n",
    "val data = spark.read.format(\"libsvm\").load(\"file:///home/dr_wang1982/infs3208/data/mllib/sample_libsvm_data.txt\")\n",
    "//data.collect.foreach(println)\n",
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(data)\n",
    "// Automatically identify categorical features, and index them.\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4) // features with > 4 distinct values are treated as continuous.\n",
    "  .fit(data)\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n",
    "//trainingData.collect.foreach(println)\n",
    "// Train a DecisionTree model.\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "// Convert indexed labels back to original labels.\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)\n",
    "\n",
    "// Chain indexers and tree in a Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")\n",
    "\n",
    "val treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(s\"Learned classification tree model:\\n ${treeModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6248737134600261\n",
      "Cluster Centers: \n",
      "[9.1,9.1,9.1]\n",
      "[0.05,0.05,0.05]\n",
      "[0.2,0.2,0.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
       "dataset: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_e6b75063ccf1\n",
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_e6b75063ccf1, k=3, distanceMeasure=euclidean, numFeatures=3\n",
       "predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_00fac50e87ba, metricName=silhouette, distanceMeasure=squaredEuclidean\n",
       "silhouette: Double = 0.6248737134600261\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 3: Clustering\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "\n",
    "// Loads data.\n",
    "val dataset = spark.read.format(\"libsvm\").load(\"mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "// Trains a k-means model.\n",
    "val kmeans = new KMeans().setK(3).setSeed(1L)\n",
    "val model = kmeans.fit(dataset)\n",
    "\n",
    "// Make predictions\n",
    "val predictions = model.transform(dataset)\n",
    "\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|          items|freq|\n",
      "+---------------+----+\n",
      "|        [Bread]|   4|\n",
      "|       [Diaper]|   4|\n",
      "|[Diaper, Bread]|   3|\n",
      "|         [Milk]|   4|\n",
      "| [Milk, Diaper]|   3|\n",
      "|  [Milk, Bread]|   3|\n",
      "|         [Beer]|   3|\n",
      "| [Beer, Diaper]|   3|\n",
      "+---------------+----+\n",
      "\n",
      "+----------+----------+----------+------+\n",
      "|antecedent|consequent|confidence|  lift|\n",
      "+----------+----------+----------+------+\n",
      "|    [Beer]|  [Diaper]|       1.0|  1.25|\n",
      "|  [Diaper]|   [Bread]|      0.75|0.9375|\n",
      "|  [Diaper]|    [Milk]|      0.75|0.9375|\n",
      "|  [Diaper]|    [Beer]|      0.75|  1.25|\n",
      "|   [Bread]|  [Diaper]|      0.75|0.9375|\n",
      "|   [Bread]|    [Milk]|      0.75|0.9375|\n",
      "|    [Milk]|  [Diaper]|      0.75|0.9375|\n",
      "|    [Milk]|   [Bread]|      0.75|0.9375|\n",
      "+----------+----------+----------+------+\n",
      "\n",
      "+------+---------------+\n",
      "| items|     prediction|\n",
      "+------+---------------+\n",
      "|[Milk]|[Diaper, Bread]|\n",
      "+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.fpm.FPGrowth\n",
       "dataset: org.apache.spark.sql.DataFrame = [items: array<string>]\n",
       "testDataset: org.apache.spark.sql.DataFrame = [items: array<string>]\n",
       "fpgrowth: org.apache.spark.ml.fpm.FPGrowth = fpgrowth_adf0b1a46230\n",
       "model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_adf0b1a46230, numTrainingRecords=5\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 5: Frequent Pattern Mining (Basket Analysis)\n",
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "\n",
    "val dataset = spark.createDataset(Seq(\n",
    "  \"Milk Bread\",\n",
    "  \"Bread Diaper Beer Eggs\",\n",
    "  \"Milk Diaper Beer Coke\",\n",
    "\"Bread Milk Diaper Beer\",\n",
    "\"Bread Milk Diaper Coke\")\n",
    ").map(t => t.split(\" \")).toDF(\"items\")\n",
    "\n",
    "val testDataset = spark.createDataset(Seq(\"Milk\")).map(t => t.split(\" \")).toDF(\"items\")\n",
    "\n",
    "val fpgrowth = new FPGrowth().setItemsCol(\"items\").setMinSupport(0.6).setMinConfidence(0.6)\n",
    "val model = fpgrowth.fit(dataset)\n",
    "\n",
    "// Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "// Display generated association rules.\n",
    "model.associationRules.show()\n",
    "// transform examines the input items against all the association rules and summarize the\n",
    "// consequents as prediction\n",
    "model.transform(testDataset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
