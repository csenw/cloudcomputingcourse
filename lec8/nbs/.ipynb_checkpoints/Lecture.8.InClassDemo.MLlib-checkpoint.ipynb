{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input path does not exist: file:/home/jovyan/work/nbs/breastcancer.txt",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/jovyan/work/nbs/breastcancer.txt",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)",
      "  at org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)",
      "  at org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithNumSlots(DAGScheduler.scala:435)",
      "  at org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:388)",
      "  at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:358)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:468)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)",
      "  at scala.collection.SetLike.map(SetLike.scala:104)",
      "  at scala.collection.SetLike.map$(SetLike.scala:104)",
      "  at scala.collection.mutable.AbstractSet.map(Set.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:467)",
      "  at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:454)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:986)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2160)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)",
      "  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2938)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)",
      "  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2938)",
      "  at org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)",
      "  at org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)",
      "  at org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "// ML Demo 1: logistic regression\n",
    "\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "/**\n",
    "*  Missing values show up as a dot.  The dot function below\n",
    "*  returns -1 if there is a dot or blank value. And it converts strings to double.\n",
    "* Then later we will\n",
    "*  delete all rows that have any -1 values.\n",
    "*/\n",
    "def dot (s: String) : Double = {\n",
    "if (s.contains(\".\") || s.length == 0) {\n",
    "return -1\n",
    "} else {\n",
    "return s.toDouble\n",
    "}\n",
    "}\n",
    "// To create DataFrame\n",
    "// Step 1. create an RDD of Rows\n",
    "val readingsRDD = sc.textFile(\"breastcancer.txt\")\n",
    "val RDD = readingsRDD.map(_.split(\",\"))\n",
    "val rowRDD = RDD.map(s => Row(dot(s(0)),dot(s(1)),dot(s(2)),dot(s(3)),dot(s(4)),dot(s(5)),dot(s(6)),\n",
    "dot(s(7)),dot(s(8)),dot(s(9)),dot(s(10)),dot(s(11)),dot(s(12)),\n",
    "dot(s(13))))\n",
    "\n",
    "// Step 2. create a schema\n",
    "val schema = StructType (List(\n",
    "StructField(\"STR\", DoubleType, true),\n",
    "StructField(\"OBS\", DoubleType, true),\n",
    "StructField(\"AGMT\", DoubleType, true),\n",
    "StructField(\"FNDX\", DoubleType, true),\n",
    "StructField(\"HIGD\", DoubleType, true),\n",
    "StructField(\"DEG\",DoubleType, true),\n",
    "StructField(\"CHK\", DoubleType, true),\n",
    "StructField(\"AGP1\", DoubleType, true),\n",
    "StructField(\"AGMN\", DoubleType, true),\n",
    "StructField(\"NLV\", DoubleType, true),\n",
    "StructField(\"LIV\", DoubleType, true),\n",
    "StructField(\"WT\", DoubleType, true),\n",
    "StructField(\"AGLP\", DoubleType, true),\n",
    "StructField(\"MST\", DoubleType, true)))\n",
    "\n",
    "// Step 3. Apply schema to RDD of Rows\n",
    "val readingsDF = spark.createDataFrame(rowRDD, schema)\n",
    "// readingsDF.show(10)\n",
    "/**\n",
    "*  Create a new dataframe dropping all of those with missing values.\n",
    "*/\n",
    "\n",
    "//var cleanDF = readingsDF.filter(readingsDF(\"NLV\") > -1)\n",
    "var cleanDF = readingsDF.filter($\"STR\" > -1 && $\"OBS\" > -1 && $\"AGMT\" > -1 && $\"FNDX\" > -1 \n",
    "                                && $\"HIGD\" > -1 && $\"DEG\" > -1 && $\"CHK\" > -1 && $\"AGP1\" > -1\n",
    "                                && $\"AGMN\" > -1 && $\"NLV\" > -1 && $\"LIV\" > -1 && $\"WT\" > -1\n",
    "                                && $\"AGLP\" > -1 && $\"MST\" > -1)\n",
    "\n",
    "//cleanDF.show(10)\n",
    "\n",
    "/**\n",
    "*  Now comes something more complicated.  Our dataframe has the column headings\n",
    "*  we created with the schema.  But we need a column called “label” and one called\n",
    "* “features” to plug into the LR algorithm.  So we use the VectorAssembler() to do that.\n",
    "* Features is a Vector of doubles.  These are all the values like patient age, etc. that\n",
    "* we extracted above.  The label indicated whether the patient has cancer.\n",
    "*/\n",
    "val featureCols = Array(\"STR\" , \"OBS\" , \"AGMT\" , \"HIGD\" , \"DEG\" , \"CHK\" , \"AGP1\" , \"AGMN\" , \"NLV\" , \"LIV\" , \"WT\" , \"AGLP\",  \"MST\" )\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "val df2 = assembler.transform(cleanDF)\n",
    "\n",
    "\n",
    "// df2.show(10)\n",
    "\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"FNDX\").setOutputCol(\"label\")\n",
    "val df3 = labelIndexer.fit(df2).transform(df2)\n",
    "\n",
    "val Array(dfTrain,dfTest) = df3.randomSplit(Array(0.7,0.3))\n",
    "/**\n",
    "*   Now we declare the LR model and run fit and transform to make predictions.\n",
    "*/\n",
    "val lrModel = new LogisticRegression().fit(dfTrain)\n",
    "val predictions = lrModel.transform(dfTest)\n",
    "\n",
    "predictions.select (\"features\", \"label\", \"prediction\").show()\n",
    "val lrEvaluator = new BinaryClassificationEvaluator()                    \n",
    "                    .setRawPredictionCol(\"rawPrediction\")\n",
    "val p = lrEvaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC: $p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strIdx_5ae239708d0f\n",
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[98,99,100,1...|\n",
      "|           0.0|  0.0|(692,[121,122,123...|\n",
      "|           0.0|  0.0|(692,[122,123,148...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           1.0|  0.0|(692,[125,126,127...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.08333333333333337\n",
      "Learned classification tree model:\n",
      " DecisionTreeClassificationModel (uid=dtc_56e670285563) of depth 2 with 5 nodes\n",
      "  If (feature 405 <= 21.0)\n",
      "   If (feature 99 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 not in {2.0})\n",
      "    Predict: 1.0\n",
      "  Else (feature 405 > 21.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = [label: double, features: vector]\n",
       "labelIndexer = strIdx_5ae239708d0f\n",
       "featureIndexer = vecIdx_db66e2ab4645\n",
       "trainingData = [label: double, features: vector]\n",
       "testData = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.m...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 2: Decision Tree\n",
    "// with pipeline\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "// Load the data stored in LIBSVM format as a DataFrame.\n",
    "val data = spark.read.format(\"libsvm\").load(\"file:///home/dr_wang1982/infs3208/data/mllib/sample_libsvm_data.txt\")\n",
    "//data.collect.foreach(println)\n",
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(data)\n",
    "// Automatically identify categorical features, and index them.\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4) // features with > 4 distinct values are treated as continuous.\n",
    "  .fit(data)\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing).\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n",
    "//trainingData.collect.foreach(println)\n",
    "// Train a DecisionTree model.\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "// Convert indexed labels back to original labels.\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)\n",
    "\n",
    "// Chain indexers and tree in a Pipeline.\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")\n",
    "\n",
    "val treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(s\"Learned classification tree model:\\n ${treeModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6248737134600261\n",
      "Cluster Centers: \n",
      "[9.1,9.1,9.1]\n",
      "[0.05,0.05,0.05]\n",
      "[0.2,0.2,0.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
       "dataset: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_6942676c1f22\n",
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_6942676c1f22, k=3, distanceMeasure=euclidean, numFeatures=3\n",
       "predictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_606b0f145708, metricName=silhouette, distanceMeasure=squaredEuclidean\n",
       "silhouette: Double = 0.6248737134600261\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 3: Clustering\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "\n",
    "// Loads data.\n",
    "val dataset = spark.read.format(\"libsvm\").load(\"mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "// Trains a k-means model.\n",
    "val kmeans = new KMeans().setK(3).setSeed(1L)\n",
    "val model = kmeans.fit(dataset)\n",
    "\n",
    "// Make predictions\n",
    "val predictions = model.transform(dataset)\n",
    "\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Demo 4: Collaborative Filtering (Movie Ratings)\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "lines = spark.read.text(\"mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=long(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|          items|freq|\n",
      "+---------------+----+\n",
      "|        [Bread]|   4|\n",
      "|       [Diaper]|   4|\n",
      "|[Diaper, Bread]|   3|\n",
      "|         [Milk]|   4|\n",
      "| [Milk, Diaper]|   3|\n",
      "|  [Milk, Bread]|   3|\n",
      "|         [Beer]|   3|\n",
      "| [Beer, Diaper]|   3|\n",
      "+---------------+----+\n",
      "\n",
      "+----------+----------+----------+------+\n",
      "|antecedent|consequent|confidence|  lift|\n",
      "+----------+----------+----------+------+\n",
      "|    [Beer]|  [Diaper]|       1.0|  1.25|\n",
      "|  [Diaper]|   [Bread]|      0.75|0.9375|\n",
      "|  [Diaper]|    [Milk]|      0.75|0.9375|\n",
      "|  [Diaper]|    [Beer]|      0.75|  1.25|\n",
      "|   [Bread]|  [Diaper]|      0.75|0.9375|\n",
      "|   [Bread]|    [Milk]|      0.75|0.9375|\n",
      "|    [Milk]|  [Diaper]|      0.75|0.9375|\n",
      "|    [Milk]|   [Bread]|      0.75|0.9375|\n",
      "+----------+----------+----------+------+\n",
      "\n",
      "+------+---------------+\n",
      "| items|     prediction|\n",
      "+------+---------------+\n",
      "|[Milk]|[Diaper, Bread]|\n",
      "+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.fpm.FPGrowth\n",
       "dataset: org.apache.spark.sql.DataFrame = [items: array<string>]\n",
       "testDataset: org.apache.spark.sql.DataFrame = [items: array<string>]\n",
       "fpgrowth: org.apache.spark.ml.fpm.FPGrowth = fpgrowth_47e7d3095f88\n",
       "model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_47e7d3095f88, numTrainingRecords=5\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ML Demo 5: Frequent Pattern Mining (Basket Analysis)\n",
    "import org.apache.spark.ml.fpm.FPGrowth\n",
    "\n",
    "val dataset = spark.createDataset(Seq(\n",
    "  \"Milk Bread\",\n",
    "  \"Bread Diaper Beer Eggs\",\n",
    "  \"Milk Diaper Beer Coke\",\n",
    "\"Bread Milk Diaper Beer\",\n",
    "\"Bread Milk Diaper Coke\")\n",
    ").map(t => t.split(\" \")).toDF(\"items\")\n",
    "\n",
    "val testDataset = spark.createDataset(Seq(\"Milk\")).map(t => t.split(\" \")).toDF(\"items\")\n",
    "\n",
    "val fpgrowth = new FPGrowth().setItemsCol(\"items\").setMinSupport(0.6).setMinConfidence(0.6)\n",
    "val model = fpgrowth.fit(dataset)\n",
    "\n",
    "// Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "// Display generated association rules.\n",
    "model.associationRules.show()\n",
    "// transform examines the input items against all the association rules and summarize the\n",
    "// consequents as prediction\n",
    "model.transform(testDataset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
