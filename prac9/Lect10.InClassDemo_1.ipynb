{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: InClass Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD Creation\n",
    "### Note: to display the content in RDD, .collect() and .foreach(func) are used in the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// textFile() method \n",
    "val lines = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/txtDemo.txt\")\n",
    "lines.collect().foreach(println)\n",
    "\n",
    "// parallelize() method\n",
    "//val dataA = Array(1, 2, 3, 4, 5)\n",
    "//val rddA = sc.parallelize(dataA)\n",
    "\n",
    "//val dataS = List(\"MapReduce is good\",\"Spark is fast\",\"Spark is better than MapReduce\")\n",
    "//val rddS = sc.parallelize(dataS)\n",
    "\n",
    "//rddA.collect().foreach(println)\n",
    "//println(\"-------------\")    \n",
    "//rddS.collect().foreach(println)\n",
    "\n",
    "// Spark Web UI - http://35.197.180.110:8080/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// transformations\n",
    "rddA.map(x => x + 10).collect().foreach(println)\n",
    "println(\"-------------\")\n",
    "rddA.map(x => x + 10).filter(x => x > 12).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// flatMap\n",
    "rddS.collect().flatMap(l => l.split(\" \")).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "// pseudo set transformation\n",
    "val rddSet1 = sc.parallelize(List(\"coffee\", \"coffee\", \"panda\", \"monkey\", \"tea\"))\n",
    "val rddSet2 = sc.parallelize(List(\"coffee\", \"monkey\", \"kitty\"))\n",
    "\n",
    "//rddSet1.distinct().collect().foreach(println)\n",
    "//println(\"-------------\")\n",
    "//rddSet1.union(rddSet2).collect().foreach(println)\n",
    "//println(\"-------------\")\n",
    "//rddSet1.intersection(rddSet2).collect().foreach(println)\n",
    "//rddSet2.intersection(rddSet1).collect().foreach(println)\n",
    "//println(\"-------------\")\n",
    "//rddSet1.subtract(rddSet2).collect().foreach(println)\n",
    "//println(\"-------------\")\n",
    "//rddA.cartesian(rddS).collect().foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "// Action operations\n",
    "// reduce(func)\n",
    "rddA.collect().foreach(println)\n",
    "val s = rddA.reduce((x,y) => (x * y))\n",
    "println(s\"The multiplication of the RDD itself: $s\")\n",
    "println(\"-------------\")    \n",
    "// take(n)\n",
    "rddA.take(3).foreach(println)\n",
    "println(\"-------------\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rddSample = sc.parallelize(Array(1,2,3,3))\n",
    "//println(rddSample.reduce( (x,y) => x+y ))\n",
    "//rddSample.collect.foreach(println)\n",
    "//println(rddSample.count())\n",
    "//rddSample.take(2).foreach(println)\n",
    "//rddSample.top(2).foreach(println)\n",
    "//rddSample.countByValue().foreach(println)\n",
    "//rddSample.collect.foreach(println)\n",
    "println(rddSample.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Partition & repartition\n",
    "//println(rddSample.partitions.size)\n",
    "val rddSampleRePar = rddSample.repartition(1)\n",
    "println(rddSample.partitions.size)\n",
    "println(rddSampleRePar.partitions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key/Value Pair Creation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// use map()\n",
    "val rddPairS = rddS.flatMap(x => x.split(\" \")).map(x => (x,1))\n",
    "rddPairS.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// reduceByKey\n",
    "rddPairS.reduceByKey((a,b) => (a+b)).collect.foreach(println)\n",
    "\n",
    "\n",
    "val rddStudent = sc.parallelize(List(\"s123456\",\"s123456\",\"s123456\", \"s123456\", \"s654321\", \"s654321\", \"s654321\", \"s654321\", \"s654321\"))\n",
    "val rddScores = sc.parallelize(Array(78,80,65,90,80,40,50,90,80))\n",
    "println(\"-------------\")\n",
    "val rddPairScores = rddStudent.zip(rddScores)\n",
    "rddPairScores.collect.foreach(println)\n",
    "//rddPairScores.reduceByKey((a,b) => (a+b)).collect.foreach(println)\n",
    "println(\"-------------\")\n",
    "// groupByKey\n",
    "rddPairS.groupByKey().collect.foreach(println)\n",
    "//rddPairScores.groupByKey().collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// keys and values\n",
    "rddPairS.keys.collect.foreach(println)\n",
    "rddPairS.values.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sortByKey\n",
    "val rddCount = rddPairS.reduceByKey((a,b) => (a+b))\n",
    "rddCount.sortByKey().collect.foreach(print)\n",
    "println()\n",
    "rddCount.sortByKey(false).collect.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// join\n",
    "val rddStuNo = sc.parallelize(List(\"s123456\", \"s654321\"))\n",
    "val rddStuName = sc.parallelize(List(\"John\", \"Mary\"))\n",
    "val rddStuDemo = rddStuNo.zip(rddStuName)\n",
    "rddStuDemo.collect.foreach(println)\n",
    "// previously, we have an RDD (rddPairScores) has the score information\n",
    "// Let's join two RDDs\n",
    "rddStuDemo.join(rddPairScores).collect.foreach(println)\n",
    "// should be careful about the order.\n",
    "rddPairScores.join(rddStuDemo).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RDD Programming Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Note this is a simple demo without NLP pre-processing steps, such as Stop words removal, Stemming, Tokenization, etc. \n",
    "// Please refer to NLP material for a better outcome.\n",
    "val lines = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/shakespeare.txt\");\n",
    "val rddWC1 = lines.flatMap(line => line.split(\" \"))\n",
    "val rddWC2 = rddWC1.map(word => (word, 1))\n",
    "val rddCount = rddWC2.reduceByKey((a, b) => (a+b))\n",
    "\n",
    "//rddCount.collect.foreach(println)\n",
    "//lines.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey((a,b) =>(a+b)).collect.foreach(println)\n",
    "\n",
    "rddCount.sortBy(_._2,false).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Calculate averaged marks\n",
    "{\n",
    "rddPairScores.collect.foreach(println)\n",
    "println(\"-------------\")\n",
    "val rdd1 = rddPairScores.mapValues(x => (x,1))\n",
    "rdd1.collect.foreach(println)\n",
    "//rdd1.values.collect.foreach(println)\n",
    "val rdd2 = rdd1.reduceByKey((x,y) => (x._1+y._1, x._2+y._2))\n",
    "rdd2.collect.foreach(println)\n",
    "val rdd3 = rdd2.mapValues(x => x._1/x._2)\n",
    "println(\"-------------\")\n",
    "rdd3.collect.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get top value of sale records\n",
    "// data is stored in csv format: separation is \",\"\n",
    "{\n",
    "val n = 5\n",
    "val lines = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/sales.txt\")\n",
    "// lines.collect.foreach(println)\n",
    "val rdd1 = lines.map(l => l.split(\",\"))\n",
    "// rdd1.collect().foreach(println)\n",
    "val rdd2 = rdd1.map(_(3))\n",
    "val rdd3 = rdd2.map(_.toDouble)\n",
    "//rdd3.collect.foreach(println)\n",
    "val rdd4 = rdd3.sortBy(a => a,false)\n",
    "// rdd4.collect.foreach(println)\n",
    "println(\"-------------\")\n",
    "println(s\"Top $n values are:\")\n",
    "rdd4.take(n).foreach(println)\n",
    "println(\"-------------\")\n",
    "//Get Min or Max    \n",
    "rdd3.sortBy(a => a,false).take(1).foreach(println)\n",
    "rdd3.sortBy(a => a).take(1).foreach(println)\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sorting across multiple files\n",
    "{\n",
    "val lines = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/sort\",4)\n",
    "var index = 0\n",
    "lines.collect.foreach(println)\n",
    "\n",
    "println(\"sorted results\")\n",
    "println(\"-------------\")\n",
    "lines.map(a => a.toInt).sortBy(a=>a,false).collect.foreach(println)\n",
    "println(\"-------------\")    \n",
    "val rdd1 = lines.map(a => a.toInt)\n",
    "//val rdd2 = rdd1.sortByKey()\n",
    "val rdd2 = rdd1.sortBy(a => a)\n",
    "rdd2.collect.foreach(println)\n",
    "//rdd2.saveAsTextFile(\"file:///home/dr_wang1982/infs3208/data/results\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Movie Rating Example\n",
    "{\n",
    "val ratingSmall = \"file:///home/dr_wang1982/infs3208/data/movie/ratings_small.csv\"\n",
    "val moviesSmall = \"file:///home/dr_wang1982/infs3208/data/movie/movies_small.csv\"\n",
    "val moviesLatest = \"file:///home/dr_wang1982/infs3208/data/movie/movies_latest.csv\"\n",
    "// Movie Rating\n",
    "val ratingLines = sc.textFile(ratingSmall)\n",
    "//ratingLines.collect.foreach(println)\n",
    "val rdd1 = ratingLines.map(line => line.split(\",\"))\n",
    "val rdd2 = rdd1.map(x => (x(1).toInt, x(2).toDouble))\n",
    "//rdd2.collect.foreach(println)\n",
    "//rdd2.groupByKey().collect.foreach(println)\n",
    "\n",
    "val rdd3 = rdd2.groupByKey().map(d => (d._1,d._2.sum/d._2.size))\n",
    "// or use mapValues()\n",
    "//val rdd3 = rdd2.groupByKey().mapValues(ratings => ratings.sum/ratings.size)\n",
    "//rdd3.collect.foreach(println)\n",
    "\n",
    "val movieLines = sc.textFile(moviesSmall)\n",
    "// movieLines.collect.foreach(println)\n",
    "val rdd4 = movieLines.map(line => line.split(\",\")).map(x => (x(0).toInt, x(1)))\n",
    "//rdd3.join(rdd4).collect.foreach(println)\n",
    "//rdd4.join(rdd3).collect.foreach(println)\n",
    "//rdd4.join(rdd3).map(x => (x._1, x._2._1, x._2._2)).collect.foreach(println)\n",
    "val allRankings = rdd4.join(rdd3).map(x => (x._1, x._2._1, x._2._2))\n",
    "val top100 = allRankings.sortBy(x => x._3, false).take(100)\n",
    "top100.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// A shorter version\n",
    "val rdd1 = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/movie/ratings_small.csv\").map(line => line.split(\",\")).map(x => (x(1).toInt, x(2).toDouble)).groupByKey().map(d => {\n",
    "    val avg = d._2.sum/d._2.size\n",
    "    (d._1,avg)\n",
    "    })\n",
    "val top100 = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/movie/movies_small.csv\").map(line => line.split(\",\")).map(x => (x(0).toInt, x(1))).join(rdd1).map(x => (x._1, x._2._1, x._2._2)).sortBy(x => x._3, false).take(100)\n",
    "top100.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// One-line version\n",
    "val top100 = sc.textFile(\"file:///home/dr_wang1982/infs3208/data/movie/movies_small.csv\").map(line => line.split(\",\")).map(x => (x(0).toInt, x(1))).join(sc.textFile(\"file:///home/dr_wang1982/infs3208/data/movie/ratings_small.csv\").map(line => line.split(\",\")).map(x => (x(1).toInt, x(2).toDouble)).groupByKey().map(d => (d._1,d._2.sum/d._2.size))).map(x => (x._1, x._2._1, x._2._2)).sortBy(x => x._3, false).take(100)\n",
    "top100.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark2.4.3 - Scala",
   "language": "scala",
   "name": "spark2.4.3_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}