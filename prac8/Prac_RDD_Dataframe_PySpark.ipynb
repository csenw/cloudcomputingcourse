{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. RDD Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an RDD?\n",
    "RDD (Resilient Distributed Dataset) is the fundamental data structure of Apache Spark. It represents an immutable distributed collection of objects that can be processed in parallel across a cluster.\n",
    "\n",
    "Key characteristics of RDDs include:\n",
    "\n",
    "+ Resilient: RDDs can recover from node failures using lineage information.\n",
    "+ Distributed: Data is split across multiple nodes in a cluster.\n",
    "+ Dataset: RDDs hold data that can be operated on using functional transformations like map, filter, and actions like reduce, collect.\n",
    "RDDs are fault-tolerant and lazily evaluated, meaning transformations are only executed when an action is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD Creation\n",
    "There are two different ways to create RDDs. \n",
    "\n",
    "1. You can create an RDD by reading data from an external file, such as a text file, using textFile().\n",
    "\n",
    "2. You can create an RDD from a Python collection (like a list) using the parallelize() method.\n",
    "\n",
    "Please use the given document and collections to create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The University of Queensland (UQ) is a public research university located primarily in Brisbane, the capital city of the Australian state of Queensland. Founded in 1909 by the state parliament, UQ is one of the six sandstone universities, an informal designation of the oldest university in each state. The University of Queensland is ranked second nationally by Excellence in Research for Australia and equal second in Australia based on the average of four major global university league tables. The University of Queensland is a founding member of edX, Australia's research-intensive Group of Eight, the international research network McDonnell International Scholars Academy, and the global Universitas 21 network.\n",
      "\n",
      "The main St Lucia campus occupies much of the riverside inner suburb of St Lucia, southwest of the Brisbane central business district. Other UQ campuses and facilities are located throughout Queensland, the largest of which are the Gatton campus and the Mayne Medical School. UQ's overseas establishments include UQ North America office in Washington D.C., and the UQ-Ochsner Clinical School in Louisiana, United States.\n",
      "\n",
      "The university offers associate, bachelor, master, doctoral, and higher doctorate degrees through a college, a graduate school, and six faculties. UQ incorporates over one hundred research institutes and centres, such as the Institute for Molecular Bioscience, Boeing Research and Technology Australia Centre, the Australian Institute for Bioengineering and Nanotechnology, and the UQ Dow Centre for Sustainable Engineering Innovation. Recent research achievements of the university include pioneering the invention of the HPV vaccine that prevents cervical cancer, developing a COVID-19 vaccine currently in human trials and the development of high-performance superconducting MRI magnets for portable scanning of human limbs.\n",
      "\n",
      "UQ counts two Nobel laureates (Peter C. Doherty and John Harsanyi), over a hundred Olympians winning numerous gold medals and 117 Rhodes Scholars among its alumni and staff. UQ's alumni also include the President of the University of California San Francisco Sam Hawgood, the first female Governor-General of Australia Dame Quentin Bryce, President of King's College London Ed Byrne, member of United Kingdom's Prime Minister Council for Science and Technology Max Lu, Oscar and Emmy awards winner Geoffrey Rush, triple Grammy Award winner Tim Munro, former Chief Justices of Australia, and the former CEO and Chairman of Dow Chemical and current Director of DowDuPont Andrew N. Liveris. \n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a file\n",
    "# Input your code here:\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize a SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Load the text file into an RDD\n",
    "rdd_from_file = sc.textFile(\"uq.txt\")\n",
    "\n",
    "# To verify the data is loaded, you can use the collect() action to print the contents\n",
    "for line in rdd_from_file.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The University of Queensland (UQ) is a public research university located primarily in Brisbane, the capital city of the Australian state of Queensland. Founded in 1909 by the state parliament, UQ is one of the six sandstone universities, an informal designation of the oldest university in each state. The University of Queensland is ranked second nationally by Excellence in Research for Australia and equal second in Australia based on the average of four major global university league tables. The University of Queensland is a founding member of edX, Australia's research-intensive Group of Eight, the international research network McDonnell International Scholars Academy, and the global Universitas 21 network.\n",
      "The main St Lucia campus occupies much of the riverside inner suburb of St Lucia, southwest of the Brisbane central business district. Other UQ campuses and facilities are located throughout Queensland, the largest of which are the Gatton campus and the Mayne Medical School. UQ's overseas establishments include UQ North America office in Washington D.C., and the UQ-Ochsner Clinical School in Louisiana, United States.\n",
      "The university offers associate, bachelor, master, doctoral, and higher doctorate degrees through a college, a graduate school, and six faculties. UQ incorporates over one hundred research institutes and centres, such as the Institute for Molecular Bioscience, Boeing Research and Technology Australia Centre, the Australian Institute for Bioengineering and Nanotechnology, and the UQ Dow Centre for Sustainable Engineering Innovation. Recent research achievements of the university include pioneering the invention of the HPV vaccine that prevents cervical cancer, developing a COVID-19 vaccine currently in human trials and the development of high-performance superconducting MRI magnets for portable scanning of human limbs.\n",
      "UQ counts two Nobel laureates (Peter C. Doherty and John Harsanyi), over a hundred Olympians winning numerous gold medals and 117 Rhodes Scholars among its alumni and staff. UQ's alumni also include the President of the University of California San Francisco Sam Hawgood, the first female Governor-General of Australia Dame Quentin Bryce, President of King's College London Ed Byrne, member of United Kingdom's Prime Minister Council for Science and Technology Max Lu, Oscar and Emmy awards winner Geoffrey Rush, triple Grammy Award winner Tim Munro, former Chief Justices of Australia, and the former CEO and Chairman of Dow Chemical and current Director of DowDuPont Andrew N. Liveris.\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from an existing data structure.\n",
    "# Input your code here:\n",
    "\n",
    "# open the source file\n",
    "with open('uq.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# remove uneeded lines and convert to python List structure\n",
    "data = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "rdd_from_data = sc.parallelize(data)\n",
    "\n",
    "for line in rdd_from_data.collect():\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDD Transfermations and Actions:\n",
    "In Spark, Transformations are operations that transform an existing RDD to a new RDD.  (i.e., they are not executed until an action is triggered).\n",
    "\n",
    "###  Common RDD Transformations:\n",
    "1. `map()`: Applies a function to each element of the RDD and returns a new RDD.\n",
    "2. `filter()`: Filters elements based on a condition and returns a new RDD with elements that satisfy the condition.\n",
    "3. `flatMap()`: Similar to map(), but each input item can be mapped to multiple output items (i.e., it flattens the result).\n",
    "4. `distinct()`: Returns a new RDD containing only distinct elements.\n",
    "5. `union()`: Combines two RDDs and returns a new RDD containing all elements from both RDDs.\n",
    "\n",
    "More operations can be found in [official documents](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 30\n",
      "of: 26\n",
      "and: 20\n",
      "university: 9\n",
      "in: 8\n",
      "uq: 7\n",
      "research: 6\n",
      "a: 6\n",
      "for: 6\n",
      "queensland: 5\n",
      "australia: 5\n",
      "is: 4\n",
      "state: 3\n",
      "school: 3\n",
      "include: 3\n",
      "brisbane: 2\n",
      "global: 2\n",
      "network: 2\n",
      "scholars: 2\n",
      "st: 2\n",
      "lucia: 2\n",
      "campus: 2\n",
      "are: 2\n",
      "united: 2\n",
      "college: 2\n",
      "institute: 2\n",
      "technology: 2\n",
      "vaccine: 2\n",
      "human: 2\n",
      "alumni: 2\n",
      "winner: 2\n",
      "located: 2\n",
      "australian: 2\n",
      "by: 2\n",
      "one: 2\n",
      "six: 2\n",
      "second: 2\n",
      "member: 2\n",
      "international: 2\n",
      "uq's: 2\n",
      "over: 2\n",
      "hundred: 2\n",
      "centre: 2\n",
      "dow: 2\n",
      "president: 2\n",
      "former: 2\n",
      "public: 1\n",
      "primarily: 1\n",
      "1909: 1\n",
      "parliament: 1\n",
      "sandstone: 1\n",
      "an: 1\n",
      "informal: 1\n",
      "oldest: 1\n",
      "ranked: 1\n",
      "nationally: 1\n",
      "excellence: 1\n",
      "equal: 1\n",
      "based: 1\n",
      "four: 1\n",
      "major: 1\n",
      "league: 1\n",
      "founding: 1\n",
      "australia's: 1\n",
      "group: 1\n",
      "eight: 1\n",
      "mcdonnell: 1\n",
      "21: 1\n",
      "occupies: 1\n",
      "riverside: 1\n",
      "central: 1\n",
      "business: 1\n",
      "other: 1\n",
      "gatton: 1\n",
      "medical: 1\n",
      "overseas: 1\n",
      "america: 1\n",
      "uq-ochsner: 1\n",
      "offers: 1\n",
      "bachelor: 1\n",
      "master: 1\n",
      "doctoral: 1\n",
      "higher: 1\n",
      "doctorate: 1\n",
      "degrees: 1\n",
      "graduate: 1\n",
      "faculties: 1\n",
      "institutes: 1\n",
      "centres: 1\n",
      "as: 1\n",
      "molecular: 1\n",
      "nanotechnology: 1\n",
      "sustainable: 1\n",
      "engineering: 1\n",
      "achievements: 1\n",
      "invention: 1\n",
      "hpv: 1\n",
      "prevents: 1\n",
      "cervical: 1\n",
      "developing: 1\n",
      "covid: 1\n",
      "19: 1\n",
      "trials: 1\n",
      "development: 1\n",
      "high-performance: 1\n",
      "superconducting: 1\n",
      "magnets: 1\n",
      "portable: 1\n",
      "scanning: 1\n",
      "counts: 1\n",
      "two: 1\n",
      "nobel: 1\n",
      "c: 1\n",
      "john: 1\n",
      "gold: 1\n",
      "medals: 1\n",
      "rhodes: 1\n",
      "staff: 1\n",
      "hawgood: 1\n",
      "female: 1\n",
      "quentin: 1\n",
      "bryce: 1\n",
      "king's: 1\n",
      "ed: 1\n",
      "byrne: 1\n",
      "kingdom's: 1\n",
      "minister: 1\n",
      "science: 1\n",
      "max: 1\n",
      "lu: 1\n",
      "oscar: 1\n",
      "awards: 1\n",
      "rush: 1\n",
      "award: 1\n",
      "chief: 1\n",
      "justices: 1\n",
      "ceo: 1\n",
      "chairman: 1\n",
      "current: 1\n",
      "andrew: 1\n",
      "liveris: 1\n",
      "capital: 1\n",
      "city: 1\n",
      "founded: 1\n",
      "universities: 1\n",
      "designation: 1\n",
      "each: 1\n",
      "on: 1\n",
      "average: 1\n",
      "tables: 1\n",
      "edx: 1\n",
      "research-intensive: 1\n",
      "academy: 1\n",
      "universitas: 1\n",
      "main: 1\n",
      "much: 1\n",
      "inner: 1\n",
      "suburb: 1\n",
      "southwest: 1\n",
      "district: 1\n",
      "campuses: 1\n",
      "facilities: 1\n",
      "throughout: 1\n",
      "largest: 1\n",
      "which: 1\n",
      "mayne: 1\n",
      "establishments: 1\n",
      "north: 1\n",
      "office: 1\n",
      "washington: 1\n",
      "d.c.: 1\n",
      "clinical: 1\n",
      "louisiana: 1\n",
      "states: 1\n",
      "associate: 1\n",
      "through: 1\n",
      "incorporates: 1\n",
      "such: 1\n",
      "bioscience: 1\n",
      "boeing: 1\n",
      "bioengineering: 1\n",
      "innovation: 1\n",
      "recent: 1\n",
      "pioneering: 1\n",
      "that: 1\n",
      "cancer: 1\n",
      "currently: 1\n",
      "mri: 1\n",
      "limbs: 1\n",
      "laureates: 1\n",
      "peter: 1\n",
      "doherty: 1\n",
      "harsanyi: 1\n",
      "olympians: 1\n",
      "winning: 1\n",
      "numerous: 1\n",
      "117: 1\n",
      "among: 1\n",
      "its: 1\n",
      "also: 1\n",
      "california: 1\n",
      "san: 1\n",
      "francisco: 1\n",
      "sam: 1\n",
      "first: 1\n",
      "governor-general: 1\n",
      "dame: 1\n",
      "london: 1\n",
      "prime: 1\n",
      "council: 1\n",
      "emmy: 1\n",
      "geoffrey: 1\n",
      "triple: 1\n",
      "grammy: 1\n",
      "tim: 1\n",
      "munro: 1\n",
      "chemical: 1\n",
      "director: 1\n",
      "dowdupont: 1\n",
      "n: 1\n"
     ]
    }
   ],
   "source": [
    "# Transfermation\n",
    "# Case 1: word count\n",
    "# Split the uq.txt document into individual words, and get the word count of the document.\n",
    "# Input your code here:\n",
    "import re\n",
    "def tokenize_text(text):\n",
    "    tokens = re.findall(r\"\\b(?:[A-Za-z]\\.){2,}|\\b[A-Za-z]+(?:['-][A-Za-z]+)*|\\d+(?:-\\d+)*\\b\", text.lower())\n",
    "    # TODO: apply more advanced tokenizer methods here\n",
    "    return tokens\n",
    "\n",
    "# Split each line into words using flatMap\n",
    "words_rdd = rdd_from_file.flatMap(tokenize_text)\n",
    "\n",
    "# Map each word to (word, 1) for counting\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count the occurrences of each word\n",
    "word_count_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the results in descending order of frequency\n",
    "sorted_word_count_rdd = word_count_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Collect and display the results\n",
    "for word, count in sorted_word_count_rdd.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "#Stop the SparkContext Created in Block 1\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original collection [6, 10, 9, 3, 10, 6, 9, 8, 6, 10, 4, 8, 10, 5, 10, 9, 4, 3, 0, 4]\n",
      "final collection with add 10 and filter > 12 [16, 20, 19, 13, 20, 16, 19, 18, 16, 20, 14, 18, 20, 15, 20, 19, 14, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: collection operation\n",
    "# Code below is to generate a ramdom integer collection \n",
    "import random\n",
    "collection = [random.randint(0, 10) for _ in range(20)]\n",
    "print(\"original collection\", collection)\n",
    "# Firstly, add each element in a collection by 10; secondly, display the elements that are greater than 12.\n",
    "# Your Code Here:\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(collection)\n",
    "\n",
    "# 1. Add 10 to each element using map\n",
    "rdd_added = rdd.map(lambda x: x + 10)\n",
    "\n",
    "# 2. Filter elements greater than 12\n",
    "rdd_filtered = rdd_added.filter(lambda x: x > 12)\n",
    "\n",
    "# Collect and show the results\n",
    "result = rdd_filtered.collect()\n",
    "print(\"final collection with add 10 and filter > 12\",result)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct elements in RDD1:\n",
      "['infs3208', 'infs3202', 'infs7208', 'infs7202']\n",
      "Distinct elements in RDD2:\n",
      "['infs3208', 'infs7204', 'infs3204', 'infs7208']\n",
      "Common elements between RDD1 and RDD2:\n",
      "['infs3208', 'infs7208']\n",
      "Elements in RDD1 but not in RDD2:\n",
      "['infs3202', 'infs7202']\n",
      "Cartesian product between RDD1 and RDD2:\n",
      "[('infs3202', 'infs3208'), ('infs3202', 'infs3208'), ('infs7208', 'infs3208'), ('infs7208', 'infs3208'), ('infs3202', 'infs3204'), ('infs3202', 'infs7204'), ('infs7208', 'infs3204'), ('infs7208', 'infs7204'), ('infs3202', 'infs7208'), ('infs7208', 'infs7208'), ('infs3208', 'infs3208'), ('infs3208', 'infs3208'), ('infs7202', 'infs3208'), ('infs7202', 'infs3208'), ('infs3208', 'infs3208'), ('infs3208', 'infs3208'), ('infs3208', 'infs3204'), ('infs3208', 'infs7204'), ('infs7202', 'infs3204'), ('infs7202', 'infs7204'), ('infs3208', 'infs7208'), ('infs7202', 'infs7208'), ('infs3208', 'infs3204'), ('infs3208', 'infs7204'), ('infs3208', 'infs7208')]\n"
     ]
    }
   ],
   "source": [
    "# Case 3: set operation\n",
    "\"\"\"\n",
    " Given two different sets:\n",
    "    s1 = (\"infs3202\", \"infs7208\", \"infs3208\", \"infs7202\", \"infs3208\")\n",
    "    s2 = (\"infs3208\", \"infs3208\", \"infs3204\", \"infs7204\", \"infs7208\")\n",
    "    1. Create two RDDs using parallelize method\n",
    "    2. Display all distinct elements in all the RDDs.\n",
    "    3. Display all common elements between two RDDs.\n",
    "    4. Subtract the first RDD with the second RDD and display the result.\n",
    "    5. Display all the cartesian products between the first RDD and the second RDD:\n",
    "\"\"\"\n",
    "# Input your code here:\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Given sets\n",
    "s1 = (\"infs3202\", \"infs7208\", \"infs3208\", \"infs7202\", \"infs3208\")\n",
    "s2 = (\"infs3208\", \"infs3208\", \"infs3204\", \"infs7204\", \"infs7208\")\n",
    "\n",
    "# 1. Create two RDDs using parallelize method\n",
    "rdd1 = sc.parallelize(s1)\n",
    "rdd2 = sc.parallelize(s2)\n",
    "\n",
    "# 2. Display all distinct elements in all RDDs\n",
    "distinct_rdd1 = rdd1.distinct()\n",
    "distinct_rdd2 = rdd2.distinct()\n",
    "\n",
    "print(\"Distinct elements in RDD1:\")\n",
    "print(distinct_rdd1.collect())\n",
    "\n",
    "print(\"Distinct elements in RDD2:\")\n",
    "print(distinct_rdd2.collect())\n",
    "\n",
    "# 3. Display all common elements between two RDDs\n",
    "common_elements = rdd1.intersection(rdd2)\n",
    "\n",
    "print(\"Common elements between RDD1 and RDD2:\")\n",
    "print(common_elements.collect())\n",
    "\n",
    "# 4. Subtract the first RDD with the second RDD and display the result\n",
    "rdd_subtracted = rdd1.subtract(rdd2)\n",
    "\n",
    "print(\"Elements in RDD1 but not in RDD2:\")\n",
    "print(rdd_subtracted.collect())\n",
    "\n",
    "# 5. Display all the Cartesian products between the first RDD and the second RDD\n",
    "cartesian_rdd = rdd1.cartesian(rdd2)\n",
    "\n",
    "print(\"Cartesian product between RDD1 and RDD2:\")\n",
    "print(cartesian_rdd.collect())\n",
    "\n",
    "# Stop the Spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RDD Actions\n",
    "Actions in RDDs are operations that trigger the execution of transformations and return a result (or store the result).\n",
    "1. `collect()`: Collects all elements of the RDD and returns them as a list to the driver program.\n",
    "2. `count()`: Returns the number of elements in the RDD.\n",
    "3. `first()`: Returns the first element in the RDD.\n",
    "4. `reduce()`: Aggregates the elements of the RDD using a specified function.\n",
    "5. `take(n)`: Returns the first n elements of the RDD.\n",
    "\n",
    "More operations can be found [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original collection [1, 5, 3, 3, 3, 6, 5, 3, 6, 4, 2, 4, 1, 4, 2, 1, 5, 5, 5, 1]\n",
      "Factorial of each element in the RDD:\n",
      "[1, 120, 6, 6, 6, 720, 120, 6, 720, 24, 2, 24, 1, 24, 2, 1, 120, 120, 120, 1]\n",
      "Number of elements in the RDD: 20\n",
      "Randomly selected 4 elements: [5, 6, 3, 3]\n",
      "Two largest elements in the RDD: [6, 6]\n",
      "Frequency of each element in the RDD:\n",
      "1: 4\n",
      "5: 5\n",
      "3: 4\n",
      "6: 2\n",
      "4: 3\n",
      "2: 2\n"
     ]
    }
   ],
   "source": [
    "# Action operations\n",
    "# Given below code to generate a ramdom collection\n",
    "import random\n",
    "collection = [random.randint(1, 6) for _ in range(20)]\n",
    "\n",
    "# 1. Get the factorial result of the RDD\n",
    "# 2. Count the elements in the RDD\n",
    "# 3. Ramdomly select 4 elements out of the RDD, and print them out.\n",
    "# 4. Display the two largest elements in the RDD\n",
    "# 5. Check the frequency of each element in the RDD\n",
    "# Input your code here:\n",
    "\n",
    "# Create RDD from the collection\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(collection)\n",
    "print(\"original collection\", rdd.collect())\n",
    "\n",
    "# 1. Get the factorial result of the RDD (factorial of each element)\n",
    "from math import factorial\n",
    "factorial_rdd = rdd.map(lambda x: factorial(x))\n",
    "\n",
    "print(\"Factorial of each element in the RDD:\")\n",
    "print(factorial_rdd.collect())\n",
    "\n",
    "# 2. Count the elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"Number of elements in the RDD:\", count)\n",
    "\n",
    "\n",
    "# 3. Randomly select 4 elements out of the RDD, and print them out\n",
    "sampled_elements = rdd.takeSample(False, 4)\n",
    "print(\"Randomly selected 4 elements:\", sampled_elements)\n",
    "\n",
    "# 4. Display the two largest elements in the RDD\n",
    "largest_elements = rdd.takeOrdered(2, key=lambda x: -x)\n",
    "print(\"Two largest elements in the RDD:\", largest_elements)\n",
    "\n",
    "# 5. Check the frequency of each element in the RDD\n",
    "element_frequencies = rdd.countByValue()\n",
    "print(\"Frequency of each element in the RDD:\")\n",
    "for element, frequency in element_frequencies.items():\n",
    "    print(f\"{element}: {frequency}\")\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key/Value Pair Creation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:\n",
      "MapReduce: 2\n",
      "is: 3\n",
      "good: 1\n",
      "Spark: 2\n",
      "than: 1\n",
      "fast: 1\n",
      "better: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Case 1:\n",
    "Given a List of String:\n",
    "[\"MapReduce is good\",\"Spark is fast\",\"Spark is better than MapReduce\"]\n",
    " \n",
    "create an RDD and count the word frequency using transformation operations, followed by displaying the results using action operations. \n",
    "\"\"\"\n",
    "# Input your code here:\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "data = [\"MapReduce is good\", \"Spark is fast\", \"Spark is better than MapReduce\"]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "word_count_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_count_result = word_count_rdd.collect()\n",
    "print(\"Word frequencies:\")\n",
    "for word, count in word_count_result:\n",
    "    print(f\"{word}: {count}\")\n",
    "    \n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores for each student:\n",
      "s123456: 78.25\n",
      "s654321: 68.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Case2:\n",
    "Given scores of two studends below:\n",
    "    \"s123456\",78\n",
    "    \"s123456\",80\n",
    "    \"s123456\",65\n",
    "    \"s123456\",90\n",
    "    \"s654321\",80\n",
    "    \"s654321\",40\n",
    "    \"s654321\",50\n",
    "    \"s654321\",90 \n",
    "    \"s654321\" 80 \n",
    " \n",
    " store the data into an RDD and calculate the average scores for each student.\n",
    "\"\"\"\n",
    "#Input your code here:\n",
    "\n",
    "# Given scores for two students\n",
    "data = [\n",
    "    (\"s123456\", 78),\n",
    "    (\"s123456\", 80),\n",
    "    (\"s123456\", 65),\n",
    "    (\"s123456\", 90),\n",
    "    (\"s654321\", 80),\n",
    "    (\"s654321\", 40),\n",
    "    (\"s654321\", 50),\n",
    "    (\"s654321\", 90),\n",
    "    (\"s654321\", 80)\n",
    "]\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Map each student and score to (student_id, (score, 1)) pair to keep track of the sum and count\n",
    "student_scores = rdd.map(lambda x: (x[0], (x[1], 1)))\n",
    "# Reduce by key to sum the scores and the counts for each student\n",
    "student_totals = student_scores.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "# Calculate the average by dividing total score by the count\n",
    "student_averages = student_totals.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "# Action: Collect the result and print the average scores\n",
    "average_scores = student_averages.collect()\n",
    "print(\"Average scores for each student:\")\n",
    "for student, avg in average_scores:\n",
    "    print(f\"{student}: {avg:.2f}\")\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. PySQL DataFrame Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DataFrame?\n",
    "A DataFrame in PySpark is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a dataframe in Python’s pandas library, but optimized for large-scale distributed computing.\n",
    "\n",
    "Key characteristics of DataFrames:\n",
    "\n",
    "+ Schema: DataFrames have a schema, meaning each column has a name and a type, which makes it easy to work with structured data.\n",
    "+ Distributed: Like RDDs, DataFrames are distributed across a cluster.\n",
    "+ Optimized: DataFrames benefit from Spark’s Catalyst Optimizer, which optimizes query execution, and Tungsten engine, which improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential functions for next operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, desc\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataframe Creation\n",
    "\n",
    "There are two different ways to create Dataframes. \n",
    "\n",
    "1. You can create a DataFrame from an RDD by specifying a schema.\n",
    "\n",
    "2. You can also create a DataFrame by reading data from files like CSV, JSON, or Hadoop Parquet.\n",
    "\n",
    "Please use the given document and collections to create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The University of...|\n",
      "|                    |\n",
      "|The main St Lucia...|\n",
      "|                    |\n",
      "|The university of...|\n",
      "|                    |\n",
      "|UQ counts two Nob...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create from files\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TextFileToDataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "df = spark.read.text(\"uq.txt\")\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|The University of...|\n",
      "|The main St Lucia...|\n",
      "|The university of...|\n",
      "|UQ counts two Nob...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Create from a schema\n",
    "# open the source file\n",
    "with open('uq.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# remove uneeded lines and convert to python List data structure\n",
    "data = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ListToDataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the text file and process it into a list\n",
    "with open('uq.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove unneeded lines and strip whitespace\n",
    "data = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Create a PySpark DataFrame from the list\n",
    "df = spark.createDataFrame([(line,) for line in data], [\"text\"])\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataframe Transfermations and Actions:\n",
    "\n",
    "PySpark Dataframe also follows the Transform and Action mode.\n",
    "\n",
    "Transformations in DataFrames are similar to SQL-like operations and are also lazily evaluated. They return a new DataFrame and are optimized by the Catalyst Optimizer.\n",
    "\n",
    "### Common DataFrame Transformations:\n",
    "\n",
    "1. `select()`: Selects specific columns from the DataFrame.\n",
    "2. `filter() / where()`: Filters rows based on a condition.\n",
    "3. `groupBy()`: Groups the DataFrame by a specific column(s) and can be used with aggregate functions.\n",
    "4. `withColumn()`: Adds or modifies a column.\n",
    "5. `join()`: Joins two DataFrames on a specified column.\n",
    "6. `orderBy()`: Sorts the DataFrame by the specified column(s).\n",
    "\n",
    "Actions in DataFrames trigger the execution of transformations and return results to the driver program.\n",
    "\n",
    "### Common DataFrame Actions:\n",
    "\n",
    "1. `show()`: Displays the content of the DataFrame in tabular format.\n",
    "2. `collect()`: Returns all the data in the DataFrame as a list to the driver program.\n",
    "3. `count()`: Returns the number of rows in the DataFrame.\n",
    "4. `first()`: Returns the first row of the DataFrame.\n",
    "5. `take(n)`: Returns the first n rows as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|word      |count|\n",
      "+----------+-----+\n",
      "|the       |30   |\n",
      "|of        |26   |\n",
      "|and       |20   |\n",
      "|university|9    |\n",
      "|in        |8    |\n",
      "|uq        |7    |\n",
      "|research  |6    |\n",
      "|for       |6    |\n",
      "|a         |6    |\n",
      "|australia |5    |\n",
      "|queensland|5    |\n",
      "|is        |4    |\n",
      "|include   |3    |\n",
      "|school    |3    |\n",
      "|state     |3    |\n",
      "|technology|2    |\n",
      "|six       |2    |\n",
      "|former    |2    |\n",
      "|united    |2    |\n",
      "|human     |2    |\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transfermation in Dataframe\n",
    "# Case 1: word count\n",
    "# Split the uq.txt document into individual words, and get the word count of the document.\n",
    "# Input your code here:\n",
    "import re\n",
    "\n",
    "# Define your custom tokenization function\n",
    "def tokenize_text(text):\n",
    "    tokens = re.findall(r\"\\b(?:[A-Za-z]\\.){2,}|\\b[A-Za-z]+(?:['-][A-Za-z]+)*|\\d+(?:-\\d+)*\\b\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Register the tokenize_text function as a UDF\n",
    "tokenize_udf = F.udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# Use the custom tokenize_text function to split text into words\n",
    "# Apply the UDF and split the text into tokens (words)\n",
    "tokenized_df = df.withColumn(\"words\", tokenize_udf(col(\"text\")))\n",
    "\n",
    "# # Explode the array of words to individual rows\n",
    "words_df = tokenized_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Group by each word and count the occurrences\n",
    "word_count_df = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Sort the results in descending order of frequency\n",
    "sorted_word_count_df = word_count_df.orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "sorted_word_count_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original collection [3, 0, 10, 1, 2, 4, 5, 1, 6, 5, 9, 9, 6, 0, 6, 8, 9, 10, 0, 10]\n",
      "+------+------------+\n",
      "|number|added_number|\n",
      "+------+------------+\n",
      "|     3|          13|\n",
      "|    10|          20|\n",
      "|     4|          14|\n",
      "|     5|          15|\n",
      "|     6|          16|\n",
      "|     5|          15|\n",
      "|     9|          19|\n",
      "|     9|          19|\n",
      "|     6|          16|\n",
      "|     6|          16|\n",
      "|     8|          18|\n",
      "|     9|          19|\n",
      "|    10|          20|\n",
      "|    10|          20|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 2: collection operation with Dataframe\n",
    "# Code below is to generate a ramdom integer collection \n",
    "import random\n",
    "collection = [random.randint(0, 10) for _ in range(20)]\n",
    "print(\"original collection\", collection)\n",
    "\n",
    "# We can operate collection with SparkSQL DataFrame\n",
    "# Firstly, add each element in a collection by 10; secondly, display the elements that are greater than 12.\n",
    "# Input your code here:\n",
    "\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"CollectionTransformation\").getOrCreate()\n",
    "\n",
    "# The collection of random integers\n",
    "# random_integers = [98, 2, 48, 66, 7, 83, 90, 37, 81, 69, 96, 99, 98, 71, 73, 32, 74, 71, 97, 16]\n",
    "\n",
    "# Create a DataFrame from the collection\n",
    "df = spark.createDataFrame([(i,) for i in collection], [\"number\"])\n",
    "\n",
    "# Add 10 to each element\n",
    "df_added = df.withColumn(\"added_number\", df[\"number\"] + 10)\n",
    "\n",
    "# Filter elements that are greater than 12\n",
    "df_filtered = df_added.filter(df_added[\"added_number\"] > 12)\n",
    "\n",
    "# Show the results\n",
    "df_filtered.show()\n",
    "\n",
    "# Stop the Spark session after operation\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct elements in df1:\n",
      "+--------+\n",
      "| subject|\n",
      "+--------+\n",
      "|infs3208|\n",
      "|infs3202|\n",
      "|infs7208|\n",
      "|infs7202|\n",
      "+--------+\n",
      "\n",
      "Distinct elements in df2:\n",
      "+--------+\n",
      "| subject|\n",
      "+--------+\n",
      "|infs3208|\n",
      "|infs7204|\n",
      "|infs3204|\n",
      "|infs7208|\n",
      "+--------+\n",
      "\n",
      "Common elements between df1 and df2:\n",
      "+--------+\n",
      "| subject|\n",
      "+--------+\n",
      "|infs3208|\n",
      "|infs7208|\n",
      "+--------+\n",
      "\n",
      "Elements in df1 but not in df2:\n",
      "+--------+\n",
      "| subject|\n",
      "+--------+\n",
      "|infs3202|\n",
      "|infs7202|\n",
      "+--------+\n",
      "\n",
      "Cartesian product of df1 and df2:\n",
      "+--------+--------+\n",
      "|subject |subject |\n",
      "+--------+--------+\n",
      "|infs3202|infs3208|\n",
      "|infs3202|infs3208|\n",
      "|infs7208|infs3208|\n",
      "|infs7208|infs3208|\n",
      "|infs3202|infs3204|\n",
      "|infs3202|infs7204|\n",
      "|infs3202|infs7208|\n",
      "|infs7208|infs3204|\n",
      "|infs7208|infs7204|\n",
      "|infs7208|infs7208|\n",
      "|infs3208|infs3208|\n",
      "|infs3208|infs3208|\n",
      "|infs7202|infs3208|\n",
      "|infs7202|infs3208|\n",
      "|infs3208|infs3208|\n",
      "|infs3208|infs3208|\n",
      "|infs3208|infs3204|\n",
      "|infs3208|infs7204|\n",
      "|infs3208|infs7208|\n",
      "|infs7202|infs3204|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 3: set operation with Dataframe\n",
    "\"\"\"\n",
    " Given two different sets:\n",
    "    s1 = (\"infs3202\", \"infs7208\", \"infs3208\", \"infs7202\", \"infs3208\")\n",
    "    s2 = (\"infs3208\", \"infs3208\", \"infs3204\", \"infs7204\", \"infs7208\")\n",
    "    1. Create two DataFrames\n",
    "    2. Display all distinct elements in both the df.\n",
    "    3. Display all common elements between two df.\n",
    "    4. Subtract the first df with the second df and display the result.\n",
    "    5. Display all the cartesian products between the first df and the second df:\n",
    "\"\"\"\n",
    "# Input your code here:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SetOperations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "s1 = (\"infs3202\", \"infs7208\", \"infs3208\", \"infs7202\", \"infs3208\")\n",
    "s2 = (\"infs3208\", \"infs3208\", \"infs3204\", \"infs7204\", \"infs7208\")\n",
    "\n",
    "# 1: Create two DataFrames\n",
    "df1 = spark.createDataFrame([(x,) for x in s1], [\"subject\"])\n",
    "df2 = spark.createDataFrame([(x,) for x in s2], [\"subject\"])\n",
    "\n",
    "# 2: Display all distinct elements in both DataFrames\n",
    "distinct_df1 = df1.select(\"subject\").distinct()\n",
    "distinct_df2 = df2.select(\"subject\").distinct()\n",
    "\n",
    "print(\"Distinct elements in df1:\")\n",
    "distinct_df1.show()\n",
    "\n",
    "print(\"Distinct elements in df2:\")\n",
    "distinct_df2.show()\n",
    "\n",
    "# 3: Display all common elements between two DataFrames\n",
    "common_elements_df = distinct_df1.intersect(distinct_df2)\n",
    "\n",
    "print(\"Common elements between df1 and df2:\")\n",
    "common_elements_df.show()\n",
    "\n",
    "# 4: Subtract the first DataFrame with the second DataFrame and display the result\n",
    "subtract_df = distinct_df1.subtract(distinct_df2)\n",
    "\n",
    "print(\"Elements in df1 but not in df2:\")\n",
    "subtract_df.show()\n",
    "\n",
    "# 5: Display all the Cartesian products between the two DataFrames\n",
    "cartesian_product_df = df1.crossJoin(df2)\n",
    "\n",
    "print(\"Cartesian product of df1 and df2:\")\n",
    "cartesian_product_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorial result of each element:\n",
      "+------+---------+\n",
      "|number|factorial|\n",
      "+------+---------+\n",
      "|2     |2        |\n",
      "|2     |2        |\n",
      "|6     |720      |\n",
      "|4     |24       |\n",
      "|1     |1        |\n",
      "|1     |1        |\n",
      "|2     |2        |\n",
      "|5     |120      |\n",
      "|5     |120      |\n",
      "|5     |120      |\n",
      "|3     |6        |\n",
      "|3     |6        |\n",
      "|1     |1        |\n",
      "|4     |24       |\n",
      "|3     |6        |\n",
      "|4     |24       |\n",
      "|5     |120      |\n",
      "|5     |120      |\n",
      "|4     |24       |\n",
      "|3     |6        |\n",
      "+------+---------+\n",
      "\n",
      "Count of elements in the DataFrame: 20\n",
      "Randomly selected 4 elements:\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|5     |\n",
      "|3     |\n",
      "|1     |\n",
      "|2     |\n",
      "+------+\n",
      "\n",
      "Two largest elements:\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     6|\n",
      "|     5|\n",
      "+------+\n",
      "\n",
      "Frequency of each element:\n",
      "+------+---------+\n",
      "|number|frequency|\n",
      "+------+---------+\n",
      "|5     |5        |\n",
      "|3     |4        |\n",
      "|4     |4        |\n",
      "|2     |3        |\n",
      "|1     |3        |\n",
      "|6     |1        |\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Action operations with Dataframe (DF)\n",
    "# Given below code to generate a ramdom collection\n",
    "import random\n",
    "import math\n",
    "collection = [random.randint(1, 6) for _ in range(20)]\n",
    "\n",
    "# 1. Get the factorial result of the DF\n",
    "# 2. Count the elements in the DF\n",
    "# 3. Ramdomly select 4 elements out of the DF, and print them out.\n",
    "# 4. Display the two largest elements in the DF\n",
    "# 5. Check the frequency of each element in the DF\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RandomCollectionOperations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate random collection and create a DataFrame\n",
    "collection = [random.randint(1, 6) for _ in range(20)]\n",
    "df = spark.createDataFrame([(x,) for x in collection], [\"number\"])\n",
    "\n",
    "# 1: Get the factorial result of the DF\n",
    "\n",
    "def factorial(n):\n",
    "    return math.factorial(n)\n",
    "\n",
    "factorial_udf = F.udf(factorial, IntegerType())\n",
    "df_with_factorial = df.withColumn(\"factorial\", factorial_udf(col(\"number\")))\n",
    "print(\"Factorial result of each element:\")\n",
    "df_with_factorial.show(truncate=False)\n",
    "\n",
    "# 2: Count the elements in the DataFrame\n",
    "element_count = df.count()\n",
    "print(f\"Count of elements in the DataFrame: {element_count}\")\n",
    "\n",
    "# 3: Randomly select 4 elements from the DataFrame and print them\n",
    "random_elements = df.orderBy(F.rand()).limit(4)\n",
    "print(\"Randomly selected 4 elements:\")\n",
    "random_elements.show(truncate=False)\n",
    "\n",
    "# 4: Display the two largest elements in the DataFrame\n",
    "two_largest_elements = df.orderBy(col(\"number\").desc()).limit(2)\n",
    "print(\"Two largest elements:\")\n",
    "two_largest_elements.show()\n",
    "\n",
    "# 5: Check the frequency of each element in the DataFrame\n",
    "from pyspark.sql.functions import count as _count\n",
    "\n",
    "frequency_df = df.groupBy(\"number\").agg(_count(\"number\").alias(\"frequency\")).orderBy(desc(\"frequency\"))\n",
    "print(\"Frequency of each element:\")\n",
    "frequency_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key/Value Pair Creation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|word     |count|\n",
      "+---------+-----+\n",
      "|is       |3    |\n",
      "|MapReduce|2    |\n",
      "|Spark    |2    |\n",
      "|than     |1    |\n",
      "|better   |1    |\n",
      "|fast     |1    |\n",
      "|good     |1    |\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given a List of String:\n",
    "[\"MapReduce is good\",\"Spark is fast\",\"Spark is better than MapReduce\"]\n",
    " \n",
    "create an DataFrame and count the word frequency using transformation operations, followed by displaying the results using action operations. \n",
    "\"\"\"\n",
    "# Input your code here:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordFrequency\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Given list of strings\n",
    "data = [(\"MapReduce is good\",), \n",
    "        (\"Spark is fast\",), \n",
    "        (\"Spark is better than MapReduce\",)]\n",
    "\n",
    "# Step 1: Create a DataFrame from the list of strings\n",
    "df = spark.createDataFrame(data, [\"sentence\"])\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "# Step 2: Split each sentence into words (transformation operation)\n",
    "# We use the split() function instead of UDF to split each sentence into an array of words\n",
    "words_df = df.select(explode(split(col(\"sentence\"), \"\\s+\")).alias(\"word\"))\n",
    "\n",
    "# Step 3: Group by word and count occurrences (transformation operation)\n",
    "word_count_df = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Step 4: Sort the words by frequency in descending order (optional, transformation)\n",
    "sorted_word_count_df = word_count_df.orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Step 5: Display the results (action operation)\n",
    "sorted_word_count_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|student_id|average_score|\n",
      "+----------+-------------+\n",
      "|s123456   |78.25        |\n",
      "|s654321   |68.0         |\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Given scores of two studends below:\n",
    "    \"s123456\",78\n",
    "    \"s123456\",80\n",
    "    \"s123456\",65\n",
    "    \"s123456\",90\n",
    "    \"s654321\",80\n",
    "    \"s654321\",40\n",
    "    \"s654321\",50\n",
    "    \"s654321\",90 \n",
    "    \"s654321\" 80 \n",
    " \n",
    " store the data into a DataFrame and calculate the average scores for each student.\n",
    "\"\"\"\n",
    "# Input your code here:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentAverageScores\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"s123456\", 78),\n",
    "    (\"s123456\", 80),\n",
    "    (\"s123456\", 65),\n",
    "    (\"s123456\", 90),\n",
    "    (\"s654321\", 80),\n",
    "    (\"s654321\", 40),\n",
    "    (\"s654321\", 50),\n",
    "    (\"s654321\", 90),\n",
    "    (\"s654321\", 80)\n",
    "]\n",
    "\n",
    "# Create DataFrame with columns 'student_id' and 'score'\n",
    "df = spark.createDataFrame(data, [\"student_id\", \"score\"])\n",
    "\n",
    "# Group by student_id and calculate the average score for each student\n",
    "average_scores_df = df.groupBy(\"student_id\").agg(F.avg(\"score\").alias(\"average_score\"))\n",
    "\n",
    "# Show the results\n",
    "average_scores_df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the key differences between RDD and DataFrame in PySpark:\n",
    "\n",
    "| **Aspect**              | **RDD (Resilient Distributed Dataset)**                      | **DataFrame**                                             |\n",
    "|-------------------------|--------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Data Structure**       | Distributed collection of objects                            | Distributed collection of data organized into named columns |\n",
    "| **Schema**               | No schema (unstructured)                                     | Has a schema (structured with column names and data types)  |\n",
    "| **Optimization**         | No built-in optimizations, manual control over operations    | Optimized by Spark's Catalyst Optimizer and Tungsten engine |\n",
    "| **Ease of Use**          | Lower-level API, more verbose                                | Higher-level API, more user-friendly, SQL-like syntax       |\n",
    "| **Operations**           | Functional transformations (`map()`, `filter()`, etc.)       | SQL-like operations (`select()`, `groupBy()`, `agg()`, etc.) |\n",
    "| **Performance**          | Slower, no automatic optimization                            | Faster, due to query optimization and better memory management |\n",
    "| **Error Handling**       | More difficult to debug due to lack of schema                | Easier to debug with clear schema and error messages        |\n",
    "| **Interoperability with SQL** | Cannot directly use SQL                                  | Can use SQL queries directly on DataFrames                  |\n",
    "| **Use Cases**            | Ideal for low-level transformations, unstructured data       | Ideal for structured data, SQL-style analytics, and optimizations |\n",
    "| **Memory Usage**         | Less efficient memory usage                                  | More efficient memory usage through Tungsten engine         |\n",
    "\n",
    "### Key Points:\n",
    "- **RDD** is better suited for complex transformations with fine-grained control, but requires more manual work and lacks automatic optimizations.\n",
    "- **DataFrame** is more user-friendly, performs better due to optimizations, and is ideal for working with structured data and running SQL queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
